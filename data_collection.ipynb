{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used car prices - Autotrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data source: https://www.autotrader.com.au/\n",
    "#### Scope: Perth WA, 25km radius from postcode 6000\n",
    "#### Pages scraped from the site: 251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: The below code takes up to 12 hours running in a regular computer with regular internet connection !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store car data\n",
    "car_data_set = []\n",
    "\n",
    "# Set the number of pages you want to scrape\n",
    "starting_page = 0\n",
    "num_pages = 200\n",
    "\n",
    "# Scrape data from multiple pages\n",
    "print('Starting web scraping')\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "current_page = starting_page\n",
    "\n",
    "while current_page < starting_page + num_pages:\n",
    "    \n",
    "    try:\n",
    "        page = current_page\n",
    "\n",
    "        # Set up Splinter\n",
    "        driver_path = os.path.join(os.path.expanduser(\"~\"),\"Documents\", \"DATA ANALYTICS BOOTCAMP\", \"Apps\", \"chromedriver_win32\", \"chromedriver.exe\")\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--incognito\")\n",
    "\n",
    "        browser = Browser('chrome', service=Service(executable_path=driver_path), options=chrome_options)\n",
    "\n",
    "        # Visit the Autotrader site\n",
    "        url = f'https://www.autotrader.com.au/for-sale/wa/perth?page={(page+1)}&distance=25'\n",
    "        browser.visit(url)\n",
    "        html = browser.html\n",
    "\n",
    "        # Create a Beautiful Soup object\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Get all car listings\n",
    "        car_listing = soup.find_all('a', class_='carListing')\n",
    "\n",
    "        # Scrape car data from car listings\n",
    "        for i in range(len(car_listing)):\n",
    "            \n",
    "            # Create an empty dictionary to store individual car data\n",
    "            car_data = {}\n",
    "\n",
    "            # Scrape year model and append to car_data\n",
    "            h3 = car_listing[i].find('h3', class_='carListing--title')\n",
    "            year_model = h3.contents[0].strip()\n",
    "            car_data['Year model'] = year_model\n",
    "\n",
    "            # Click on the car listing\n",
    "            wait = WebDriverWait(browser.driver, 10)\n",
    "            car_listing = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a.carListing')))\n",
    "            car_listing[i].click()\n",
    "\n",
    "            # Wait for the details page to load\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Get the HTML of the current page after clicking the button\n",
    "            html = browser.html\n",
    "\n",
    "            # Create a Beautiful Soup object\n",
    "            soup_details = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # Scrape car spec and append to car_data\n",
    "            h1 = soup_details.find('h1', class_='title')\n",
    "            if h1 is not None:\n",
    "                car_spec = h1.get_text(strip=True)\n",
    "                car_data['Car Spec'] = car_spec\n",
    "            else:\n",
    "                car_data['Car Spec'] = \"\"\n",
    "\n",
    "            # Find the button element for the next click (see more details)\n",
    "            wait = WebDriverWait(browser.driver, 10)\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'div.vehicleDetails--sectionHeader > div.vehicleDetails--title')))\n",
    "            next_button.click()\n",
    "\n",
    "            # Scrape car details and append to car_data\n",
    "            tr = soup_details.find_all('tr', class_='table--row')\n",
    "\n",
    "            for row in tr:\n",
    "                label = row.find('td', class_='table--label')\n",
    "                value = row.find('td', class_='table--value')\n",
    "\n",
    "                label_text = label.get_text(strip=True)\n",
    "                value_text = value.get_text(strip=True)\n",
    "\n",
    "                car_data[label_text] = value_text\n",
    "\n",
    "            # Append the car data dictionary to the list\n",
    "            car_data_set.append(car_data)\n",
    "\n",
    "            # Find the button element for the back click (back to the car listings)\n",
    "            wait = WebDriverWait(browser.driver, 10)\n",
    "            back_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.detailsBackButton')))\n",
    "            back_button.click()\n",
    "\n",
    "            # Wait for the car listings page to load\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Re-find the car listings\n",
    "            car_listing = soup.find_all('a', class_='carListing')\n",
    "\n",
    "        # Quit the browser\n",
    "        browser.quit()\n",
    "\n",
    "        # Create DataFrame with the scraped data\n",
    "        car_df = pd.DataFrame(car_data_set)\n",
    "        car_df.head()\n",
    "\n",
    "        # Save the data and export data to csv file\n",
    "        car_df.to_csv(f'data_collection_output/output_from_{starting_page+1}_to_{page+1}.csv', index=False)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Page {page+1} - Status: completed and saved')\n",
    "\n",
    "        # Move to the next page\n",
    "        current_page += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred on page {current_page+1}. Restarting from the current car listing.')\n",
    "        traceback.print_exc()\n",
    "\n",
    "        print('Starting web scraping')\n",
    "        print('-------------------------------------------------------------')\n",
    "\n",
    "        # Repeat the loop on the same page\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
